{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ce83a3-dbd6-48c5-a8e7-794a5327e898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deps] Installing pillow ...\n",
      "[WARN] Ignoring unknown args: ['-f', 'C:\\\\Users\\\\highm\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-8f5e7972-3101-457d-97d2-f61d84e15548.json']\n",
      "[INFO] Wrote helper notebook: C:\\Users\\highm\\Downloads\\Sample_Payslips\\payslip_universal_redactor.ipynb\n",
      "[INFO] Tesseract OCR available: NO\n",
      "[INFO] Processing 4 rows...\n",
      "[1/4] REDACTED: Mandeep_payslip_redacted.pdf (OK, redactions=38)\n",
      "[2/4] REDACTED: Suraj_payslip_redacted.pdf (OK, redactions=51)\n",
      "[3/4] REDACTED: Rajesh_payslip_redacted.pdf (OK, redactions=20)\n",
      "[4/4] REDACTED: Harry_payslip_redacted.pdf (OK, redactions=18)\n",
      "[ERROR] Writing output Excel failed: [Errno 13] Permission denied: 'C:\\\\Users\\\\highm\\\\Downloads\\\\Sample_Payslips\\\\Payslips_redacted.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# payslip_universal_redactor.py — White masks, KEEP name>=60% (never redact),\n",
    "# keep emp_id & dates, redact only values after {Address, Amount in words, Net Pay In {INR|USD|CAD}, DOB},\n",
    "# plus free-form address block detection (bottom-left & below-name), safe merge that never crosses name,\n",
    "# clickable Excel links\n",
    "\n",
    "import argparse, sys, subprocess, pkgutil, re, json, traceback, shutil, difflib\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# ----- deps -----\n",
    "NEEDED = (\"pymupdf\", \"pandas\", \"openpyxl\", \"pillow\", \"pytesseract\")\n",
    "def _pip_install(pkg: str):\n",
    "    try:\n",
    "        if pkg not in {m.name for m in pkgutil.iter_modules()}:\n",
    "            print(f\"[deps] Installing {pkg} ...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg], stdout=subprocess.DEVNULL)\n",
    "    except Exception as e:\n",
    "        print(f\"[deps] Failed to install {pkg}: {e}\")\n",
    "        raise\n",
    "for _p in NEEDED:\n",
    "    _pip_install(_p)\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "TESS_AVAILABLE = shutil.which(\"tesseract\") is not None\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def norm_word(s: str) -> str:\n",
    "    \"\"\"Lowercase, strip non-alnum.\"\"\"\n",
    "    return re.sub(r'[^a-z0-9]+', '', (s or '').lower())\n",
    "\n",
    "def normalize_token(tok: str) -> str:\n",
    "    return (tok or '').strip().lower().replace('\\u00A0', ' ')\n",
    "\n",
    "# ---------- date detection ----------\n",
    "MONTHS = [\n",
    "    \"january\",\"jan\",\"february\",\"feb\",\"march\",\"mar\",\"april\",\"apr\",\"may\",\"june\",\"jun\",\n",
    "    \"july\",\"jul\",\"august\",\"aug\",\"september\",\"sep\",\"sept\",\"october\",\"oct\",\"november\",\"nov\",\"december\",\"dec\"\n",
    "]\n",
    "RE_DATE_ISO   = re.compile(r'^\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}$')               # 2024-09-25\n",
    "RE_DATE_DMY   = re.compile(r'^\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}$')             # 25/09/2024\n",
    "RE_DATE_DOTS1 = re.compile(r'^\\d{4}\\.\\d{1,2}\\.\\d{1,2}$')                   # 2024.09.25\n",
    "RE_DATE_DOTS2 = re.compile(r'^\\d{1,2}\\.\\d{1,2}\\.\\d{2,4}$')                 # 25.09.2024\n",
    "RE_MONTH_YEAR = re.compile(r'^(?:' + '|'.join(MONTHS) + r')\\W*\\d{2,4}$', re.I)\n",
    "RE_YEAR_MONTH = re.compile(r'^\\d{4}\\W*(?:' + '|'.join(MONTHS) + r')$', re.I)\n",
    "RE_RANGE_ONE  = re.compile(r'^\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\s*[-–]\\s*\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}$', re.I)\n",
    "RE_TIME       = re.compile(r'^\\d{1,2}:\\d{2}(:\\d{2})?$')\n",
    "RE_NUMERIC    = re.compile(r'\\d')\n",
    "RE_COMPACT_YMD = re.compile(r'^\\d{8}$')  # 20250630\n",
    "RE_YYYY = re.compile(r'^\\d{4}$'); RE_MM = re.compile(r'^\\d{2}$'); RE_DD = re.compile(r'^\\d{2}$')\n",
    "\n",
    "PAY_LABELS = {\"payment date\", \"payment date:\", \"pay end date\", \"pay end date:\"}\n",
    "\n",
    "def looks_like_month_name(tok: str) -> bool:\n",
    "    return normalize_token(tok).strip('.,') in MONTHS\n",
    "\n",
    "def token_is_date(tok: str) -> bool:\n",
    "    t = (tok or '').strip().strip(\",.\")\n",
    "    if not t: return False\n",
    "    if RE_DATE_ISO.match(t): return True\n",
    "    if RE_DATE_DMY.match(t): return True\n",
    "    if RE_DATE_DOTS1.match(t) or RE_DATE_DOTS2.match(t): return True\n",
    "    if RE_RANGE_ONE.match(t): return True\n",
    "    if RE_TIME.match(t): return True\n",
    "    if looks_like_month_name(t): return True\n",
    "    if RE_MONTH_YEAR.match(t) or RE_YEAR_MONTH.match(t): return True\n",
    "    if RE_COMPACT_YMD.match(t): return True  # 20250630\n",
    "    if re.search(r'^(?:' + '|'.join(MONTHS) + r')\\W*\\d{1,2}$', normalize_token(t), re.I): return True\n",
    "    if re.search(r'^\\d{1,2}\\W*(?:' + '|'.join(MONTHS) + r')$', normalize_token(t), re.I): return True\n",
    "    return False\n",
    "\n",
    "def sequence_is_date(tokens: List[str], start_idx: int, max_len: int = 3) -> Tuple[bool,int]:\n",
    "    # numeric triple like 2025 06 30\n",
    "    if start_idx+2 < len(tokens):\n",
    "        t0 = tokens[start_idx].strip().strip(\",.\"); t1 = tokens[start_idx+1].strip().strip(\",.\"); t2 = tokens[start_idx+2].strip().strip(\",.\")\n",
    "        if RE_YYYY.match(t0) and RE_MM.match(t1) and RE_DD.match(t2):\n",
    "            return True, 3\n",
    "    for L in range(1, max_len+1):\n",
    "        seq = tokens[start_idx:start_idx+L]\n",
    "        if not seq: continue\n",
    "        joined = \" \".join(s.strip().strip(\",.\") for s in seq)\n",
    "        jnorm = normalize_token(joined)\n",
    "        if re.match(r'^\\d{1,2}\\s+(?:' + '|'.join(MONTHS) + r')(\\s+\\d{2,4})?$', jnorm, re.I): return True, L\n",
    "        if re.match(r'^(?:' + '|'.join(MONTHS) + r')\\s+\\d{1,2}(\\s*,?\\s*\\d{2,4})?$', jnorm, re.I): return True, L\n",
    "        if RE_DATE_ISO.match(joined) or RE_DATE_DMY.match(joined) or RE_RANGE_ONE.match(joined): return True, L\n",
    "    return False, 0\n",
    "\n",
    "def in_pay_label_context(tokens: List[str], idx: int) -> bool:\n",
    "    \"\"\"Keep dates when near 'PAYMENT DATE' / 'PAY END DATE' labels.\"\"\"\n",
    "    start = max(0, idx-6)\n",
    "    window = \" \".join(normalize_token(t) for t in tokens[start:idx+1])\n",
    "    for lbl in PAY_LABELS:\n",
    "        if lbl in window:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ---------- name / id ----------\n",
    "def canonicalize_name_for_fuzzy(name: str) -> str:\n",
    "    if not isinstance(name, str): return \"\"\n",
    "    toks = re.findall(r\"[A-Za-z0-9'-]+\", name)\n",
    "    return \" \".join(t.lower() for t in toks if t.strip())\n",
    "\n",
    "def name_match_ratio(tokens: List[str], idx: int, name_norm: str, min_ratio: float = 0.60) -> Tuple[bool,int]:\n",
    "    \"\"\"Fuzzy match window vs normalized Excel name, ignore single-letter initials.\"\"\"\n",
    "    if not name_norm: return (False, 0)\n",
    "    name_tokens = name_norm.split()\n",
    "    best_len, best_ratio = 0, 0.0\n",
    "    max_len = min(len(tokens)-idx, len(name_tokens)+3)\n",
    "    for L in range(1, max_len+1):\n",
    "        window = tokens[idx:idx+L]\n",
    "        cand_tokens = [norm_word(t) for t in window]\n",
    "        cand_tokens = [c for c in cand_tokens if c and len(c) > 1]  # drop single-letter initials\n",
    "        if not cand_tokens: continue\n",
    "        cand_str = \" \".join(cand_tokens)\n",
    "        r = difflib.SequenceMatcher(None, cand_str, name_norm).ratio()\n",
    "        if r > best_ratio:\n",
    "            best_ratio, best_len = r, L\n",
    "    return (best_ratio >= min_ratio and best_len > 0, best_len)\n",
    "\n",
    "def empid_in_token(token: str, empid: str) -> bool:\n",
    "    if not empid: return False\n",
    "    tid = re.sub(r'\\s+','', token or '')\n",
    "    if empid in tid: return True\n",
    "    simplified_token = re.sub(r'\\W+','', token or '').lower()\n",
    "    simplified_empid = re.sub(r'\\W+','', empid).lower()\n",
    "    return simplified_empid != \"\" and simplified_empid in simplified_token\n",
    "\n",
    "# NEW: helper used by address-under-name logic\n",
    "def fuzzy_contains_name(tokens: List[str], name_norm: str, min_ratio: float = 0.60) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the given token list (e.g., one line) fuzzy-matches the normalized\n",
    "    employee name at >= min_ratio, ignoring single-letter initials.\n",
    "    \"\"\"\n",
    "    if not name_norm or not tokens:\n",
    "        return False\n",
    "    cand_tokens = [norm_word(t) for t in tokens]\n",
    "    cand_tokens = [c for c in cand_tokens if c and len(c) > 1]  # drop single-letter initials\n",
    "    if not cand_tokens:\n",
    "        return False\n",
    "    cand_str = \" \".join(cand_tokens)\n",
    "    r = difflib.SequenceMatcher(None, cand_str, name_norm).ratio()\n",
    "    return r >= min_ratio\n",
    "\n",
    "# ---------- OCR/text words ----------\n",
    "def get_words_text_or_ocr(page) -> List[tuple]:\n",
    "    words = page.get_text(\"words\")\n",
    "    if words: return words\n",
    "    if not TESS_AVAILABLE: return []\n",
    "    # OCR fallback\n",
    "    zoom = 300 / 72.0\n",
    "    mat = fitz.Matrix(zoom, zoom)\n",
    "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    ocr = pytesseract.image_to_data(img, output_type=pytesseract.Output.DATAFRAME)\n",
    "    if ocr is None or ocr.empty: return []\n",
    "    words_like = []\n",
    "    page_rect = page.rect\n",
    "    sx = page_rect.width / pix.width\n",
    "    sy = page_rect.height / pix.height\n",
    "    for _, row in ocr.iterrows():\n",
    "        if str(row.get(\"text\",\"\")).strip()==\"\" or (isinstance(row.get(\"conf\",0),(int,float)) and row[\"conf\"]<0):\n",
    "            continue\n",
    "        left, top = row.get(\"left\",0), row.get(\"top\",0)\n",
    "        width, height = row.get(\"width\",0), row.get(\"height\",0)\n",
    "        text = str(row.get(\"text\",\"\")).strip()\n",
    "        if not text: continue\n",
    "        x0 = float(left)*sx + page_rect.x0\n",
    "        y0 = float(top)*sy + page_rect.y0\n",
    "        x1 = x0 + float(width)*sx\n",
    "        y1 = y0 + float(height)*sy\n",
    "        words_like.append((x0,y0,x1,y1,text,0,0,0))\n",
    "    return words_like\n",
    "\n",
    "# ---------- rect overlap & merge (safe around name) ----------\n",
    "def rects_overlap(a: fitz.Rect, b: fitz.Rect) -> bool:\n",
    "    return (min(a.x1, b.x1) - max(a.x0, b.x0) > 0) and (min(a.y1, b.y1) - max(a.y0, b.y0) > 0)\n",
    "\n",
    "def can_merge_rects(a: fitz.Rect, b: fitz.Rect, gap_tol: float, protected: List[fitz.Rect]) -> bool:\n",
    "    # same (approx) line, small horizontal gap\n",
    "    v_overlap = min(a.y1, b.y1) - max(a.y0, b.y0)\n",
    "    if v_overlap <= -gap_tol or b.x0 > a.x1 + gap_tol:\n",
    "        return False\n",
    "    # don't merge if the union would cross any protected (name) box\n",
    "    union = fitz.Rect(min(a.x0, b.x0), min(a.y0, b.y0), max(a.x1, b.x1), max(a.y1, b.y1))\n",
    "    for pb in protected:\n",
    "        if rects_overlap(union, pb):\n",
    "            if pb.x0 >= min(a.x0, b.x0) and pb.x1 <= max(a.x1, b.x1):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def merge_rects_safe(rects: List[fitz.Rect], protected: List[fitz.Rect], gap_tol: float = 2.0) -> List[fitz.Rect]:\n",
    "    if not rects: return []\n",
    "    rects_sorted = sorted(rects, key=lambda r: (round(r.y0,2), round(r.x0,2)))\n",
    "    out = []\n",
    "    cur = rects_sorted[0]\n",
    "    for r in rects_sorted[1:]:\n",
    "        if can_merge_rects(cur, r, gap_tol, protected):\n",
    "            cur = fitz.Rect(min(cur.x0, r.x0), min(cur.y0, r.y0), max(cur.x1, r.x1), max(cur.y1, r.y1))\n",
    "        else:\n",
    "            out.append(cur); cur = r\n",
    "    out.append(cur)\n",
    "    return out\n",
    "\n",
    "# ---------- label -> value redaction (keep label visible) ----------\n",
    "def split_label_tokens(label: str) -> List[str]:\n",
    "    return [norm_word(t) for t in re.findall(r\"[A-Za-z0-9]+\", label)]\n",
    "\n",
    "def match_label_at(tokens_norm: List[str], i: int, label_tokens: List[str]) -> int:\n",
    "    L = len(label_tokens)\n",
    "    if L == 0 or i+L > len(tokens_norm): return 0\n",
    "    return L if tokens_norm[i:i+L] == label_tokens else 0\n",
    "\n",
    "ADDRESS_LABELS = [\"address\", \"address:\", \"residential address\", \"home address\", \"mailing address\"]\n",
    "AMOUNT_WORDS_LABELS = [\"amount in words\", \"amount in words:\"]\n",
    "NETPAY_LABELS = [\"net pay in inr\", \"net pay in usd\", \"net pay in cad\"]\n",
    "DOB_LABELS = [\"dob\", \"d.o.b\", \"date of birth\", \"birth date\", \"birthdate\"]\n",
    "LABEL_GROUPS = ADDRESS_LABELS + AMOUNT_WORDS_LABELS + NETPAY_LABELS + DOB_LABELS\n",
    "LABEL_TOKENS = [(lab, split_label_tokens(lab)) for lab in LABEL_GROUPS]\n",
    "\n",
    "def collect_value_after_label_rects(words_sorted: List[tuple], label_tokens: List[Tuple[str, List[str]]]) -> List[fitz.Rect]:\n",
    "    \"\"\"Redact ONLY the value to the right of the label on the same line (label stays visible).\"\"\"\n",
    "    rects = []\n",
    "    tokens = [w[4] for w in words_sorted]\n",
    "    tokens_norm = [norm_word(t) for t in tokens]\n",
    "    i = 0\n",
    "    N = len(tokens)\n",
    "    while i < N:\n",
    "        matched = False\n",
    "        for lab, lab_tok in label_tokens:\n",
    "            L = match_label_at(tokens_norm, i, lab_tok)\n",
    "            if L > 0:\n",
    "                y0 = words_sorted[i][1]\n",
    "                last_x1 = words_sorted[i+L-1][2]\n",
    "                k = i + L\n",
    "                while k < N and abs(words_sorted[k][1] - y0) < 2.5:\n",
    "                    if words_sorted[k][0] >= last_x1 - 0.5:\n",
    "                        w = words_sorted[k]\n",
    "                        rects.append(fitz.Rect(w[0], w[1], w[2], w[3]))\n",
    "                    k += 1\n",
    "                i += L\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            i += 1\n",
    "    return rects\n",
    "\n",
    "# ---------- free-form address block detection (bottom-left & below-name) ----------\n",
    "STREET_KEYWORDS = {\n",
    "    \"st\", \"street\", \"rd\", \"road\", \"ave\", \"avenue\", \"blvd\", \"drive\", \"dr\", \"lane\", \"ln\",\n",
    "    \"court\", \"ct\", \"terrace\", \"way\", \"cres\", \"crescent\", \"parkway\", \"pkwy\", \"highway\", \"hwy\"\n",
    "}\n",
    "UNIT_KEYWORDS = {\"unit\", \"apt\", \"apartment\", \"suite\", \"ste\", \"floor\", \"fl\", \"flat\"}\n",
    "RE_STREET_LINE = re.compile(r'^\\s*\\d+[A-Za-z]?\\s+[A-Za-z0-9\\'\\.\\- ]{2,}$')\n",
    "RE_CAN_POSTAL  = re.compile(r'[A-Za-z]\\d[A-Za-z][ -]?\\d[A-Za-z]\\d')\n",
    "RE_US_ZIP      = re.compile(r'\\b\\d{5}(-\\d{4})?\\b')\n",
    "\n",
    "def group_lines(words_sorted: List[tuple], y_tol: float = 2.5):\n",
    "    \"\"\"Group tokens into line objects with text and bounding rect.\"\"\"\n",
    "    lines = []\n",
    "    if not words_sorted:\n",
    "        return lines\n",
    "    cur = [words_sorted[0]]\n",
    "    for w in words_sorted[1:]:\n",
    "        if abs(w[1] - cur[-1][1]) < y_tol:\n",
    "            cur.append(w)\n",
    "        else:\n",
    "            lines.append(cur)\n",
    "            cur = [w]\n",
    "    lines.append(cur)\n",
    "    out = []\n",
    "    for line in lines:\n",
    "        xs0 = [w[0] for w in line]; ys0 = [w[1] for w in line]; xs1 = [w[2] for w in line]; ys1 = [w[3] for w in line]\n",
    "        text = \" \".join(w[4] for w in line)\n",
    "        rect = fitz.Rect(min(xs0), min(ys0), max(xs1), max(ys1))\n",
    "        out.append({\"rect\": rect, \"text\": text, \"words\": line})\n",
    "    return out\n",
    "\n",
    "# Add this new regex near your other globals (with the other RE_* constants):\n",
    "RE_SHORT_UPPER = re.compile(r\"^[A-Z][A-Z\\s/.\\-'/]{0,24}$\")  # short, all-caps fragments like \"SHERBOURNE\", \"E ST\", \"N\"\n",
    "\n",
    "def is_address_like(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Strong signal a single line is address-like. Kept intentionally *broad* for bottom-left stubs.\n",
    "    \"\"\"\n",
    "    t = normalize_token(text)\n",
    "    # Already-strong signals you had\n",
    "    if RE_STREET_LINE.search(text): return True\n",
    "    if RE_CAN_POSTAL.search(text) or RE_US_ZIP.search(text): return True\n",
    "\n",
    "    # Presence of common street/unit tokens anywhere on the line\n",
    "    toks = [norm_word(x) for x in re.findall(r\"[A-Za-z0-9']+\", text)]\n",
    "    if any(tok in STREET_KEYWORDS for tok in toks): return True\n",
    "    if any(tok in UNIT_KEYWORDS for tok in toks): return True\n",
    "\n",
    "    # NEW: short, all-caps fragments common in tear-off stubs (e.g., \"SHERBOURNE\", \"E ST\", single-letter directions)\n",
    "    # We treat this as a *weak* address signal; the cluster logic below will require ≥2 such weak lines or one strong neighbor.\n",
    "    if RE_SHORT_UPPER.match(text) and len(toks) <= 3:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def collect_freeform_address_rects(page, words_sorted: List[tuple], name_norm: str) -> List[fitz.Rect]:\n",
    "    \"\"\"\n",
    "    Redact free-form address blocks that appear:\n",
    "      - in the few lines under the detected name (already in your logic), and\n",
    "      - in the bottom-left stub area where addresses are often broken into short all-caps lines.\n",
    "    Uses a small cluster heuristic so that weak lines like \"E ST\" or \"N\" are captured\n",
    "    when adjacent to another weak/strong address-like line. Never touches name lines.\n",
    "    \"\"\"\n",
    "    rects = []\n",
    "    page_rect = page.rect\n",
    "\n",
    "    # --- group into lines first (reuses your helper) ---\n",
    "    lines = group_lines(words_sorted)\n",
    "\n",
    "    # --- strengthened \"under name\" behavior (captures SHERBOURNE / N stubs) ---\n",
    "    name_line_idxs = []\n",
    "    for idx, L in enumerate(lines):\n",
    "        tokens = [w[4] for w in L[\"words\"]]\n",
    "        if fuzzy_contains_name(tokens, name_norm, 0.60):\n",
    "            name_line_idxs.append(idx)\n",
    "\n",
    "    for idx in name_line_idxs:\n",
    "        # Look at up to the next 6 lines and build a consecutive block of address-like lines.\n",
    "        candidate_rects = []\n",
    "        for j in range(idx + 1, min(idx + 7, len(lines))):\n",
    "            line = lines[j]\n",
    "            line_tokens = [w[4] for w in line[\"words\"]]\n",
    "\n",
    "            # Never touch name lines\n",
    "            if fuzzy_contains_name(line_tokens, name_norm, 0.60):\n",
    "                break\n",
    "\n",
    "            txt = line[\"text\"]\n",
    "            # Treat as address-like if:\n",
    "            #  - our existing strong detector says so, OR\n",
    "            #  - it's a short all-caps fragment (RE_SHORT_UPPER), OR\n",
    "            #  - it contains any digit (common with unit/house numbers)\n",
    "            is_addrish = (\n",
    "                is_address_like(txt)\n",
    "                or RE_SHORT_UPPER.match(txt) is not None\n",
    "                or bool(re.search(r\"\\d\", txt))\n",
    "            )\n",
    "\n",
    "            # We collect *consecutive* address-like lines; stop on the first non-address line.\n",
    "            if is_addrish:\n",
    "                candidate_rects.append(line[\"rect\"])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # If we collected anything, redact as a single tidy block\n",
    "        if candidate_rects:\n",
    "            r = fitz.Rect(\n",
    "                min(rr.x0 for rr in candidate_rects),\n",
    "                min(rr.y0 for rr in candidate_rects),\n",
    "                max(rr.x1 for rr in candidate_rects),\n",
    "                max(rr.y1 for rr in candidate_rects),\n",
    "            )\n",
    "            rects.append(r)\n",
    "\n",
    "    # --- bottom-left candidate lines ---\n",
    "    bottom_left = []\n",
    "    for line in lines:\n",
    "        # bottom ~58% of page, left ~65% width — slightly wider than before to catch far-left text\n",
    "        midy = 0.5 * (line[\"rect\"].y0 + line[\"rect\"].y1)\n",
    "        if midy >= page_rect.y0 + 0.42 * page_rect.height and line[\"rect\"].x0 <= page_rect.x0 + 0.65 * page_rect.width:\n",
    "            bottom_left.append(line)\n",
    "\n",
    "    if not bottom_left:\n",
    "        return rects\n",
    "\n",
    "    # --- cluster adjacent lines by small vertical gaps (so \"SHERBOURNE\" + \"N\" get grouped) ---\n",
    "    def cluster_lines(lines_in, max_gap=7.0):\n",
    "        clusters = []\n",
    "        cur = [lines_in[0]]\n",
    "        for L in lines_in[1:]:\n",
    "            if abs(L[\"rect\"].y0 - cur[-1][\"rect\"].y0) < max_gap:\n",
    "                cur.append(L)\n",
    "            else:\n",
    "                clusters.append(cur); cur = [L]\n",
    "        clusters.append(cur)\n",
    "        return clusters\n",
    "\n",
    "    clusters = cluster_lines(bottom_left, max_gap=7.0)\n",
    "\n",
    "    # --- decide cluster-level addressness ---\n",
    "    for cluster in clusters:\n",
    "        # Count lines with strong signals vs weak all-caps snippets\n",
    "        strong = 0\n",
    "        weak_caps = 0\n",
    "        has_digit = 0\n",
    "        for L in cluster:\n",
    "            txt = L[\"text\"]\n",
    "            if is_address_like(txt):\n",
    "                strong += 1\n",
    "            if RE_SHORT_UPPER.match(txt):\n",
    "                weak_caps += 1\n",
    "            if re.search(r\"\\d\", txt):\n",
    "                has_digit += 1\n",
    "\n",
    "        # Cluster is address if:\n",
    "        #   (a) any strong line present, or\n",
    "        #   (b) at least two weak all-caps fragments, or\n",
    "        #   (c) one weak all-caps fragment + a neighbor that contains digits (house/unit number)\n",
    "        cluster_is_address = (strong >= 1) or (weak_caps >= 2) or (weak_caps >= 1 and has_digit >= 1)\n",
    "\n",
    "        if not cluster_is_address:\n",
    "            continue\n",
    "\n",
    "        # Build a union rect of all *non-name* lines in the cluster\n",
    "        cluster_rects = []\n",
    "        for L in cluster:\n",
    "            if not fuzzy_contains_name([w[4] for w in L[\"words\"]], name_norm, 0.60):\n",
    "                cluster_rects.append(L[\"rect\"])\n",
    "\n",
    "        if not cluster_rects:\n",
    "            continue\n",
    "\n",
    "        # Join into a single rectangular block (cleaner coverage)\n",
    "        r = fitz.Rect(\n",
    "            min(rr.x0 for rr in cluster_rects),\n",
    "            min(rr.y0 for rr in cluster_rects),\n",
    "            max(rr.x1 for rr in cluster_rects),\n",
    "            max(rr.y1 for rr in cluster_rects),\n",
    "        )\n",
    "        rects.append(r)\n",
    "\n",
    "    return rects\n",
    "\n",
    "\n",
    "# ---------- main targeting ----------\n",
    "def tokens_to_redact(page, name_norm: str, empid: str) -> List[fitz.Rect]:\n",
    "    words = get_words_text_or_ocr(page)\n",
    "    if not words: return []\n",
    "    words_sorted = sorted(words, key=lambda w: (round(w[1],1), round(w[0],1)))\n",
    "    tokens = [w[4] for w in words_sorted]\n",
    "    tokens_norm = [norm_word(t) for t in tokens]\n",
    "\n",
    "    rects: List[fitz.Rect] = []\n",
    "    name_boxes: List[fitz.Rect] = []  # protect these from overlap/merge\n",
    "\n",
    "    # A) Protect all name occurrences (we never redact names)\n",
    "    i = 0; N = len(tokens)\n",
    "    while i < N:\n",
    "        keep_name, name_len = name_match_ratio(tokens, i, name_norm, 0.60)\n",
    "        if keep_name and name_len > 0:\n",
    "            for kk in range(i, i+name_len):\n",
    "                w = words_sorted[kk]\n",
    "                name_boxes.append(fitz.Rect(w[0], w[1], w[2], w[3]))\n",
    "            i += name_len\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # B) Label → value redactions (label text itself stays)\n",
    "    rects += collect_value_after_label_rects(words_sorted, LABEL_TOKENS)\n",
    "\n",
    "    # C) Free-form address blocks (bottom-left & lines below name)\n",
    "    rects += collect_freeform_address_rects(page, words_sorted, name_norm)\n",
    "\n",
    "    # D) Numeric redactions (keep dates and emp_id)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        # keep multi-token dates\n",
    "        is_seq_date, seq_len = sequence_is_date(tokens, i, max_len=3)\n",
    "        if is_seq_date:\n",
    "            i += seq_len\n",
    "            continue\n",
    "        tok = tokens[i].strip()\n",
    "        if RE_NUMERIC.search(tok):\n",
    "            # keep single-token dates or dates near pay labels\n",
    "            if token_is_date(tok) or in_pay_label_context(tokens, i):\n",
    "                i += 1\n",
    "                continue\n",
    "            # keep emp_id\n",
    "            if empid and empid_in_token(tok, empid):\n",
    "                i += 1\n",
    "                continue\n",
    "            # redact numeric token\n",
    "            w = words_sorted[i]\n",
    "            rects.append(fitz.Rect(w[0], w[1], w[2], w[3]))\n",
    "        i += 1\n",
    "\n",
    "    # E) Remove any box that overlaps the protected name boxes, then merge safely\n",
    "    rects = [r for r in rects if not any(rects_overlap(r, nb) for nb in name_boxes)]\n",
    "    rects = merge_rects_safe(rects, protected=name_boxes, gap_tol=2.0)\n",
    "    return rects\n",
    "\n",
    "def redact_page(page, rects: List[fitz.Rect], fill=(1,1,1)):\n",
    "    \"\"\"WHITE masks as requested.\"\"\"\n",
    "    if not rects: return 0\n",
    "    for r in rects:\n",
    "        pad = 0.8\n",
    "        rr = fitz.Rect(r.x0 - pad, r.y0 - pad, r.x1 + pad, r.y1 + pad)\n",
    "        page.add_redact_annot(rr, fill=fill)\n",
    "    page.apply_redactions()\n",
    "    return len(rects)\n",
    "\n",
    "def process_pdf(pdf_path: Path, emp_name: str, empid: str, out_path: Path, verbose: bool=False) -> Tuple[bool,str]:\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "    except Exception as e:\n",
    "        return False, f\"ERROR opening PDF: {e}\"\n",
    "\n",
    "    name_norm = canonicalize_name_for_fuzzy(emp_name)\n",
    "    any_redactions = 0\n",
    "    for pno in range(len(doc)):\n",
    "        page = doc[pno]\n",
    "        rects = tokens_to_redact(page, name_norm=name_norm, empid=str(empid or \"\"))\n",
    "        n = redact_page(page, rects, fill=(1,1,1))\n",
    "        any_redactions += n\n",
    "        if verbose:\n",
    "            print(f\"  page {pno+1}: redactions={n}\")\n",
    "\n",
    "    try:\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        doc.save(str(out_path))\n",
    "        doc.close()\n",
    "        return True, f\"OK, redactions={any_redactions}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"ERROR saving redacted PDF: {e}\"\n",
    "\n",
    "# ---------- Excel helpers ----------\n",
    "def detect_columns(df: pd.DataFrame):\n",
    "    \"\"\"Robust detection of emp_id, emp_name, payslip link columns.\"\"\"\n",
    "    norm_to_orig = {str(c).strip().lower(): c for c in df.columns}\n",
    "    empid_col = None; empname_col = None; payslip_col = None\n",
    "    empid_keys   = {\"empid\",\"emp_id\",\"employeeid\",\"employee_id\",\"id\"}\n",
    "    empname_keys = {\"empname\",\"emp_name\",\"employee\",\"employee_name\",\"name\"}\n",
    "    payslip_keys = {\"payslip\",\"payslip_link\",\"paysliplink\",\"payslip_path\",\"file\",\"file_path\",\"pdf\",\"payslip-link\"}\n",
    "    for k, orig in norm_to_orig.items():\n",
    "        if empid_col   is None and k in empid_keys:   empid_col   = orig\n",
    "        if empname_col is None and k in empname_keys: empname_col = orig\n",
    "        if payslip_col is None and k in payslip_keys: payslip_col = orig\n",
    "    if payslip_col is None:\n",
    "        for orig in df.columns:\n",
    "            ser = df[orig].astype(str)\n",
    "            if ser.str.contains(r'\\.pdf($|\\?)', case=False, na=False).any() or \\\n",
    "               ser.str.contains(r'^https?://', case=False, na=False).any():\n",
    "                payslip_col = orig; break\n",
    "    return empid_col, empname_col, payslip_col\n",
    "\n",
    "def hyperlinkize_output(excel_path: Path, link_column_name: str = \"payslip_redacted_link\"):\n",
    "    \"\"\"Make the link column clickable (file:///...).\"\"\"\n",
    "    try:\n",
    "        wb = load_workbook(excel_path)\n",
    "        ws = wb.active\n",
    "        header = [c.value for c in next(ws.iter_rows(min_row=1, max_row=1))]\n",
    "        if link_column_name not in header:\n",
    "            wb.save(excel_path); return\n",
    "        col_idx = header.index(link_column_name) + 1\n",
    "        for row in ws.iter_rows(min_row=2):\n",
    "            cell = row[col_idx-1]\n",
    "            val = cell.value\n",
    "            if not isinstance(val, str): continue\n",
    "            if val.startswith(\"ERROR\"): continue\n",
    "            p = Path(val)\n",
    "            uri = (\"file:///\" + str(p.resolve()).replace(\"\\\\\",\"/\"))\n",
    "            cell.hyperlink = uri\n",
    "            cell.value = str(p.resolve())\n",
    "            cell.font = Font(color=\"0563C1\", underline=\"single\")\n",
    "        wb.save(excel_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not convert links to hyperlinks: {e}\")\n",
    "\n",
    "def write_helper_notebook(folder: Path, input_excel: Path):\n",
    "    nb = {\n",
    "      \"cells\": [{\n",
    "          \"cell_type\": \"code\",\n",
    "          \"metadata\": {},\n",
    "          \"source\": [\n",
    "            \"# Run the universal payslip redactor\\n\",\n",
    "            \"!python -m pip install pymupdf pandas openpyxl pillow pytesseract\\n\",\n",
    "            f\"!python payslip_universal_redactor.py --input \\\"{input_excel.name}\\\" --verbose\\n\"\n",
    "          ],\n",
    "          \"outputs\": [], \"execution_count\": None\n",
    "      }],\n",
    "      \"metadata\": {\"kernelspec\": {\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\n",
    "                   \"language_info\": {\"name\":\"python\"}},\n",
    "      \"nbformat\": 4, \"nbformat_minor\": 5\n",
    "    }\n",
    "    path = folder / \"payslip_universal_redactor.ipynb\"\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(nb, f, ensure_ascii=False, indent=2)\n",
    "        return True, str(path)\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# ---------- main ----------\n",
    "def main(argv=None):\n",
    "    p = argparse.ArgumentParser(description=\"Payslip Redactor (white masks, keep name>=60%, keep emp_id & dates, label-value + address-block redaction, safe merge)\")\n",
    "    p.add_argument(\"--input\",\"-i\", default=\"Payslips.xlsx\", help=\"Input Excel/CSV. Default: Payslips.xlsx\")\n",
    "    p.add_argument(\"--sheet\",\"-s\", default=None, help=\"Sheet name (Excel)\")\n",
    "    p.add_argument(\"--out\",\"-o\", default=None, help=\"Output Excel filename (default beside input)\")\n",
    "    p.add_argument(\"--no-notebook\", action=\"store_true\", help=\"Do not write helper .ipynb\")\n",
    "    p.add_argument(\"--verbose\",\"-v\", action=\"store_true\", help=\"Verbose logging\")\n",
    "    # Accept notebook-injected flags\n",
    "    args, unknown = p.parse_known_args(argv if argv is not None else sys.argv[1:])\n",
    "    if unknown:\n",
    "        print(f\"[WARN] Ignoring unknown args: {unknown}\")\n",
    "\n",
    "    in_path = Path(args.input).expanduser().resolve()\n",
    "    if not in_path.exists():\n",
    "        print(f\"[ERROR] Input file not found: {in_path}\")\n",
    "        return 2\n",
    "\n",
    "    # read input (ALWAYS DataFrame)\n",
    "    try:\n",
    "        if in_path.suffix.lower() in (\".xls\", \".xlsx\"):\n",
    "            if args.sheet is None:\n",
    "                df = pd.read_excel(in_path, sheet_name=0, engine=\"openpyxl\")\n",
    "            else:\n",
    "                df = pd.read_excel(in_path, sheet_name=args.sheet, engine=\"openpyxl\")\n",
    "            if isinstance(df, dict):\n",
    "                if args.sheet and args.sheet in df:\n",
    "                    df = df[args.sheet]\n",
    "                else:\n",
    "                    df = next(iter(df.values()))\n",
    "        elif in_path.suffix.lower() == \".csv\":\n",
    "            df = pd.read_csv(in_path)\n",
    "        else:\n",
    "            print(\"[ERROR] Unsupported input type. Use .xlsx or .csv\")\n",
    "            return 2\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Reading input failed: {e}\")\n",
    "        return 2\n",
    "\n",
    "    empid_col, empname_col, payslip_col = detect_columns(df)\n",
    "    if not payslip_col:\n",
    "        print(\"[ERROR] Could not detect the payslip path/URL column.\")\n",
    "        print(\"Columns:\", list(df.columns))\n",
    "        return 2\n",
    "    if not empname_col:\n",
    "        print(\"[WARNING] Employee name column not detected — name keep may miss.\")\n",
    "    if not empid_col:\n",
    "        print(\"[WARNING] Employee id column not detected — emp_id may be redacted.\")\n",
    "\n",
    "    out_excel = Path(args.out).expanduser().resolve() if args.out else (in_path.parent / \"Payslips_redacted.xlsx\")\n",
    "\n",
    "    if not args.no_notebook:\n",
    "        ok, msg = write_helper_notebook(in_path.parent, in_path)\n",
    "        print(f\"[INFO] Wrote helper notebook: {msg}\" if ok else f\"[WARN] Could not write notebook: {msg}\")\n",
    "\n",
    "    results = []\n",
    "    total = len(df)\n",
    "    print(f\"[INFO] Tesseract OCR available: {'YES' if TESS_AVAILABLE else 'NO'}\")\n",
    "    print(f\"[INFO] Processing {total} rows...\")\n",
    "\n",
    "    for ridx, row in df.iterrows():\n",
    "        empid = str(row[empid_col]) if empid_col in row.index and not pd.isna(row[empid_col]) else \"\"\n",
    "        emp_name = str(row[empname_col]) if empname_col in row.index and not pd.isna(row[empname_col]) else \"\"\n",
    "        pdf_ref = row[payslip_col]\n",
    "\n",
    "        if pd.isna(pdf_ref):\n",
    "            results.append({**row.to_dict(), \"payslip_redacted_link\": \"ERROR: missing payslip link\"})\n",
    "            print(f\"[{ridx+1}/{total}] Missing payslip link; skipping\")\n",
    "            continue\n",
    "\n",
    "        pdf_path = Path(str(pdf_ref)).expanduser()\n",
    "        if not pdf_path.is_absolute():\n",
    "            pdf_path = (in_path.parent / pdf_path).resolve()\n",
    "        if not pdf_path.exists():\n",
    "            results.append({**row.to_dict(), \"payslip_redacted_link\": f\"ERROR: pdf not found ({pdf_path})\"})\n",
    "            print(f\"[{ridx+1}/{total}] PDF not found: {pdf_path}\")\n",
    "            continue\n",
    "\n",
    "        out_path = pdf_path.with_name(pdf_path.stem + \"_redacted\" + pdf_path.suffix)\n",
    "\n",
    "        try:\n",
    "            success, msg = process_pdf(pdf_path, emp_name, empid, out_path, verbose=args.verbose)\n",
    "            if success:\n",
    "                results.append({**row.to_dict(), \"payslip_redacted_link\": str(out_path)})\n",
    "                print(f\"[{ridx+1}/{total}] REDACTED: {out_path.name} ({msg})\")\n",
    "            else:\n",
    "                results.append({**row.to_dict(), \"payslip_redacted_link\": f\"ERROR: {msg}\"})\n",
    "                print(f\"[{ridx+1}/{total}] FAILED: {msg}\")\n",
    "        except Exception as e:\n",
    "            tb = traceback.format_exc()\n",
    "            results.append({**row.to_dict(), \"payslip_redacted_link\": f\"ERROR: {e}\"})\n",
    "            print(f\"[{ridx+1}/{total}] Exception: {e}\\n{tb}\")\n",
    "\n",
    "    out_df = pd.DataFrame(results)\n",
    "    try:\n",
    "        out_df.to_excel(out_excel, index=False, engine=\"openpyxl\")\n",
    "        hyperlinkize_output(out_excel, link_column_name=\"payslip_redacted_link\")\n",
    "        print(f\"[INFO] Results written to {out_excel}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Writing output Excel failed: {e}\")\n",
    "        return 3\n",
    "\n",
    "    print(\"[DONE]\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_ipython  # notebook present\n",
    "        main(None)   # run without SystemExit in notebooks\n",
    "    except NameError:\n",
    "        raise SystemExit(main(None))  # normal scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719925e-5cef-453b-aece-6c48c69899fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d25a72-84d3-436c-ae97-a769b71bd3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e4130-f63d-4247-a15d-77e5fb4d519c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bb42b-1357-4c54-9c3a-ac84996768a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708cdd01-4791-4017-bd82-a65a208c7919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb894e-6acb-468f-9d99-6afb82d7058d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2864a-198e-443e-93ef-792d6dee1fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942391f5-ea12-4d7b-9d37-c6be0c4e1311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
